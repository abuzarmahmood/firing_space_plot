
#   _  _     ___                            _         _          __  __
# _| || |_  |_ _|_ __ ___  _ __   ___  _ __| |_   ___| |_ _   _ / _|/ _|
#|_  ..  _|  | || '_ ` _ \| '_ \ / _ \| '__| __| / __| __| | | | |_| |_
#|_      _|  | || | | | | | |_) | (_) | |  | |_  \__ \ |_| |_| |  _|  _|
#  |_||_|   |___|_| |_| |_| .__/ \___/|_|   \__| |___/\__|\__,_|_| |_|
#                         |_|

import os
os.chdir('/media/bigdata/PyHMM/PyHMM/')
import numpy as np
from fake_firing import *
from scipy.misc import comb
from scipy.stats import binom
import scipy.signal
from tqdm import tqdm
from scipy.special import gamma

os.chdir('/media/bigdata/firing_space_plot/')
from ephys_data import ephys_data

from sklearn.decomposition import PCA as pca

os.chdir('/media/bigdata/pomegranate_hmm')
from blech_hmm_abu import *

plt.ion()

#  ____                _             _       _
# / ___|_ __ ___  __ _| |_ ___    __| | __ _| |_ __ _
#| |   | '__/ _ \/ _` | __/ _ \  / _` |/ _` | __/ _` |
#| |___| | |  __/ (_| | ||  __/ | (_| | (_| | || (_| |
# \____|_|  \___|\__,_|\__\___|  \__,_|\__,_|\__\__,_|

nrns = 15
trials = 15
length = 600
state_order = np.asarray([0,1]) # use one order for all trials; len = num_transitions + 1
palatability_state = 1
ceil_p = 0.05 # Maximum firing probability -> Make shit sparse
jitter_t = 20 # Jitter from mean transition times for neurons on same trial
jitter_p = 0.0
jitter_p_type = 'abs'
min_duration = 100 # Min time of 1st transition & time b/w transitions & time of 2nd transition before end
seed_num = 30

all_ber_spikes, t, p, all_p, taste_scaling =  fake_ber_firing(nrns,
                    trials,
                    length,
                    state_order,
                    palatability_state,
                    ceil_p,
                    jitter_t,
                    jitter_p,
                    jitter_p_type,
                    min_duration)

all_ber_spikes = np.asarray(all_ber_spikes)
t = np.asarray(t)

all_ber_spikes_long = all_ber_spikes[0,:,:,:]
t_long = t[0,:,:,:]
for taste in range(1,all_ber_spikes.shape[0]):
    all_ber_spikes_long = np.concatenate((all_ber_spikes_long,all_ber_spikes[taste,:,:]),axis=1)
    t_long = np.concatenate((t_long,t[taste,:,:]),axis=0)

dat = np.asarray(all_ber_spikes)[0,:,0,:]
dat_t = t[0][0,:,:]
dot_raster(dat,dat_t)

# _____ _           _
#|  ___(_)_ __   __| |
#| |_  | | '_ \ / _` |
#|  _| | | | | | (_| |
#|_|   |_|_| |_|\__,_|
#
#      _                                        _       _
#  ___| |__   __ _ _ __   __ _  ___ _ __   ___ (_)_ __ | |_ ___
# / __| '_ \ / _` | '_ \ / _` |/ _ \ '_ \ / _ \| | '_ \| __/ __|
#| (__| | | | (_| | | | | (_| |  __/ |_) | (_) | | | | | |_\__ \
# \___|_| |_|\__,_|_| |_|\__, |\___| .__/ \___/|_|_| |_|\__|___/
#                        |___/     |_|

# =============================================================================
# Likelihood of generaeting data using single firing rate
# =============================================================================

# For every trial and neuron, walk through time and calculate the probability
# that the sum of spike up to that point was generated by a uniform firing rate
# using a binomial likelihood
log_probs = np.zeros(all_ber_spikes_long.shape)
mean_rate = np.mean(all_ber_spikes_long,axis=-1)
for trial in tqdm(range(log_probs.shape[1])):
    for nrn in range(log_probs.shape[0]):
        for time in range(1,log_probs.shape[-1]):
            sum_spikes = np.sum(all_ber_spikes_long[nrn,trial,:time])
            log_probs[nrn,trial,time] = binom.logpmf(sum_spikes,
                                     time,
                                     mean_rate[nrn,trial])

# Sum likelihood across the neuronal ensemble
# Each trial has a time-series of likelihood now
# Max value of negative likelihood should indicate changepoint
trial_log_probs = -np.sum(log_probs,axis=0)

# Plot out random subset of trials with actual changepoints and 
# time-series of likelhood
num_trials = 9
chosen_trials = np.random.randint(0,all_ber_spikes_long.shape[1],num_trials)
chosen_spikes = all_ber_spikes_long[:,chosen_trials,:]
chosen_t = t_long[chosen_trials,:,:]
chosen_probs = trial_log_probs[chosen_trials,:]

square_len = np.int(np.ceil(np.sqrt(num_trials)))
fig, ax = plt.subplots(square_len,square_len)

nd_idx_objs = []
for dim in range(ax.ndim):
    this_shape = np.ones(len(ax.shape))
    this_shape[dim] = ax.shape[dim]
    nd_idx_objs.append(np.broadcast_to( np.reshape(np.arange(ax.shape[dim]),this_shape.astype('int')), ax.shape).flatten())

for trial in range(num_trials):
    plt.sca(ax[nd_idx_objs[0][trial],nd_idx_objs[1][trial]])
    raster(chosen_spikes[:,trial,:],chosen_t[trial,:,:])
    plt.plot(nrns*chosen_probs[trial,:]/np.max(chosen_probs[trial,:]))


# =============================================================================
# Independent Bernoulli HMM
# =============================================================================
train_spikes = all_ber_spikes[0,:,:,:].swapaxes(0,1).swapaxes(1,2)
train_t = t[0,:,:,:]

log_prob, state_emissions, state_transitions, posterior_proba = \
                        independent_bernoulli_hmm_implement(n_states = 4, 
                                                            threshold = 1e-6, 
                                                            seeds = 10, 
                                                            n_cpu = 30, 
                                                            spikes = train_spikes,
                                                            max_iters = 2e3)

num_trials = 4
square_len = np.int(np.ceil(np.sqrt(num_trials)))
chosen_trials = np.random.randint(0,trials,num_trials)
        
fig, ax = plt.subplots(square_len,square_len)

nd_idx_objs = []
for dim in range(ax.ndim):
    this_shape = np.ones(len(ax.shape))
    this_shape[dim] = ax.shape[dim]
    nd_idx_objs.append(np.broadcast_to( np.reshape(np.arange(ax.shape[dim]),this_shape.astype('int')), ax.shape).flatten())

for trial in range(num_trials):
    plt.sca(ax[nd_idx_objs[0][trial],nd_idx_objs[1][trial]])
    raster(data=train_spikes[chosen_trials[trial],:,:].T,
           trans_times = train_t[chosen_trials[trial],:,:],
           expected_latent_state=posterior_proba[chosen_trials[trial],:,:].T)


# =============================================================================
# Use firing rates as a compromise
# =============================================================================

def dat_imshow(x):
        """
        Decorator function for more viewable firing rate heatmaps
        """
        plt.imshow(x,interpolation='nearest',aspect='auto')

def BAKS(SpikeTimes, Time):
    
        N = len(SpikeTimes)
        a = 4
        b = N**0.8
        sumnum = 0; sumdenum = 0
        
        for i in range(N):
            numerator = (((Time-SpikeTimes[i])**2)/2 + 1/b)**(-a)
            denumerator = (((Time-SpikeTimes[i])**2)/2 + 1/b)**(-a-0.5)
            sumnum = sumnum + numerator
            sumdenum = sumdenum + denumerator
            
        if len(SpikeTimes) > 0: # Catch for no firing trials
            h = (gamma(a)/gamma(a+0.5))*(sumnum/sumdenum)
            
            FiringRate = np.zeros((len(Time)))
            for j in range(N):
                K = (1/(np.sqrt(2*np.pi)*h))*np.exp(-((Time-SpikeTimes[j])**2)/((2*h)**2))
                FiringRate = FiringRate + K
                
        else:
            FiringRate = np.zeros((len(Time)))
            
        return FiringRate

firing_long = np.zeros(all_ber_spikes_long.shape)
for nrn in tqdm(range(all_ber_spikes_long.shape[0])):
    for trial in range(all_ber_spikes_long.shape[1]):
        firing_long[nrn,trial,:] = BAKS(np.where(all_ber_spikes_long[nrn,trial,:])[0]/1000, np.linspace(0,all_ber_spikes_long.shape[-1]/1000,all_ber_spikes_long.shape[-1]))

firing_long += np.random.random(firing_long.shape)*1e-9

# Normalized rates
normal_firing_long = np.zeros(firing_long.shape)

for nrn in tqdm(range(firing_long.shape[0])):
    for trial in range(firing_long.shape[1]):
        normal_firing_long[nrn,trial,:] = firing_long[nrn,trial,:]/np.max(firing_long[nrn,trial,:])

trial = 1
plt.subplot(2,1,1)
dat_imshow(normal_firing_long[:,trial,:])
for val in np.mean(t_long[trial,:,:],axis=-1):
    plt.axvline(val,c='r')
plt.subplot(2,1,2)
raster(all_ber_spikes_long[:,trial,:],t_long[trial,:,:])



# =============================================================================
#  Change in correlation of firing after changepoint
# =============================================================================
# =============================================================================
# trans_time = np.floor(np.mean(t_long[trial,:])).astype('int')
# pre_corr = np.corrcoef(normal_firing_long[:,trial,:trans_time])
# post_corr = np.corrcoef(normal_firing_long[:,trial,trans_time:])
# plt.figure();plt.subplot(1,2,1);dat_imshow(pre_corr);plt.subplot(1,2,2);dat_imshow(post_corr)
# =============================================================================

# =============================================================================
# Pomegranate Multivariate Gaussian HMM
# =============================================================================
red_dat_pca = pca(n_components=3).fit(np.mean(normal_firing_long,axis=1).T)
red_dat = np.asarray([red_dat_pca.transform(normal_firing_long[:,trial,:].T) for trial in range(normal_firing_long.shape[1]) ])

# =============================================================================
# pre_corr = np.corrcoef(red_dat[trial,:trans_time,:].T)
# post_corr = np.corrcoef(red_dat[trial,trans_time:,:].T)
# plt.figure();plt.subplot(1,2,1);dat_imshow(pre_corr);plt.subplot(1,2,2);dat_imshow(post_corr)
# =============================================================================

plt.figure();dat_imshow(red_dat[trial,:,:].T)

# =============================================================================
# log_prob, state_emissions, state_covars, state_transitions, posterior_proba = \
#     mutlivariate_gaussian_hmm(n_states = 4, 
#                               threshold = 1e-9, 
#                               firing_rates = red_dat[:trials,:,:], 
#                               seed = 0)
# =============================================================================
    
log_prob, state_emissions, state_covars, state_transitions, posterior_proba = \
    mutlivariate_gaussian_hmm_implement(n_states = 4, 
                                         threshold = 1e-9, 
                                         seeds = 100, 
                                         n_cpu = 30, 
                                         firing_rates = red_dat[:trials,:,:])
    
# =============================================================================
# plt.figure();dat_imshow(state_emissions)
# plt.figure();
# for state in range(state_covars.shape[0]):
#     plt.subplot(1,state_covars.shape[0],state+1)
#     dat_imshow(state_covars[state,:,:])
# =============================================================================

    
trial = 0
plt.figure()
plt.subplot(4,1,1)
dat_imshow(normal_firing_long[:,trial,:])
plt.subplot(4,1,2)
dat_imshow(red_dat[trial,:,:].T)
plt.subplot(4,1,3)
plt.plot(posterior_proba[trial,:,:])
plt.subplot(4,1,4)
raster(all_ber_spikes_long[:,trial,:],t_long[trial,:,:])
    
# =============================================================================
# Multivariate Gaussian Likelihood on firing rate (captures means and correlations)
# Train model on everything upto a timepoint and test on everything after
# =============================================================================

from scipy.stats import multivariate_normal
trial = 0
these_probs = np.zeros(normal_firing_long.shape[-1])
for time in tqdm(range(100,len(these_probs),100)):
    this_pdf = multivariate_normal(np.mean(normal_firing_long[:,trial,:time],axis=-1),
                                   np.cov(normal_firing_long[:,trial,:time]))
    these_probs[time] = this_pdf.logpdf(normal_firing_long[:,trial,time:])


trial = 4
these_probs = multivariate_normal.logpdf(normal_firing_long[:,trial,100:-100].T,
                                         np.mean(normal_firing_long[:,trial,:],axis=-1),
                                         np.cov(normal_firing_long[:,trial,:]))

plt.plot(these_probs);plt.axvline(np.mean(t_long[trial,:]),c='r')

# =============================================================================
# EM Algorithm
# =============================================================================
num_changepoints = 2
#num_samples = 1000
#changepoint_set =np.sort( np.random.randint(0,length,(num_changepoints,num_samples)),axis=0)
bin_size = 1
time_marks1 = np.arange(30,150,bin_size)
time_marks2 = np.arange(150,length,bin_size)
#changepoint_list_inds = np.reshape(np.arange(len(time_marks1)*len(time_marks2)),(len(time_marks1),len(time_marks2)))[np.triu_indices(len(time_marks),1)]
#changepoint_list = [np.sort(np.random.randint(0,length,num_changepoints)) for x in range(num_samples)]
changepoint_list = [ [i,j] for i in time_marks1 for j in time_marks2]
#changepoint_list = [ changepoint_list[changepoint_list_inds[i]] for i in range(len(changepoint_list_inds))]
# Random initial conditions
firing_rate = np.random.rand(nrns,num_changepoints+1)/10

# In loop:
# 1) Find changepoint with highest likelihood for given firing rate
# 2) Use changepoint to calculate NEW firing rate

# Generate ranges using changepoint set to use for indexing states
index_list = [ [ np.arange(0,x[0]), np.arange(x[0],x[1]),np.arange(x[1],length)] for x in changepoint_list]

# Use index list to create spike list
spike_list = [ [dat[:,x[0]],dat[:,x[1]],dat[:,x[2]]] for x in index_list]
spike_sums = [ [np.sum(x[0],axis=1),np.sum(x[1],axis=1),np.sum(x[2],axis=1)] for x in spike_list ]
spike_sums_array = np.asarray(spike_sums)

# Use independent binomial probabilities for each neuron
# Total likelihood is product of indepedent probabilities

state_lens = np.asarray( [ [len(x[0]),len(x[1]),len(x[2],) ] for x in index_list] )

# Function to calculate probability of sequence given state length
# and assumed firing rates
for iters in tqdm(range(30)):
#    log_probs = np.asarray([binom.logpmf(spike_sums_array[:,:,nrn],state_lens,firing_rate[nrn,:]) for nrn in range(nrns)])
    log_probs = np.zeros(spike_sums_array.shape)
    for changepoint_set in range(spike_sums_array.shape[0]):
        for nrn in range(spike_sums_array.shape[2]):
            log_probs[changepoint_set,:,nrn] = binom.logpmf(spike_sums_array[changepoint_set,:,nrn],
                                                             state_lens[changepoint_set,:],
                                                             firing_rate[nrn,:])
            
    log_probs[log_probs == 0] = -np.inf
    total_log_probs = np.nansum(log_probs, axis = (1,2))
    
    new_changepoint_ind = np.nanargmax(total_log_probs)
    new_changepoint = changepoint_list[new_changepoint_ind]
    new_inds = [ np.arange(0,new_changepoint[0]), np.arange(new_changepoint[0],new_changepoint[1]),np.arange(new_changepoint[1],length)]
    new_spikes = [dat[:,new_inds[0]],dat[:,new_inds[1]],dat[:,new_inds[2]]]
    firing_rate = np.asarray([np.mean(new_spikes[0],axis=1),np.mean(new_spikes[1],axis=1),np.mean(new_spikes[2],axis=1)]).T

spike_sums_array[new_changepoint_ind]
state_lens[new_changepoint_ind]
log_probs[:,new_changepoint_ind,:]
