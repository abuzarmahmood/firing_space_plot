
#   _  _     ___                            _         _          __  __
# _| || |_  |_ _|_ __ ___  _ __   ___  _ __| |_   ___| |_ _   _ / _|/ _|
#|_  ..  _|  | || '_ ` _ \| '_ \ / _ \| '__| __| / __| __| | | | |_| |_
#|_      _|  | || | | | | | |_) | (_) | |  | |_  \__ \ |_| |_| |  _|  _|
#  |_||_|   |___|_| |_| |_| .__/ \___/|_|   \__| |___/\__|\__,_|_| |_|
#                         |_|

import os
os.chdir('/media/bigdata/PyHMM/PyHMM/')
import numpy as np
from fake_firing import *
from scipy.stats import binom
from tqdm import tqdm
from scipy.special import gamma

plt.ion()

#  ____                _             _       _
# / ___|_ __ ___  __ _| |_ ___    __| | __ _| |_ __ _
#| |   | '__/ _ \/ _` | __/ _ \  / _` |/ _` | __/ _` |
#| |___| | |  __/ (_| | ||  __/ | (_| | (_| | || (_| |
# \____|_|  \___|\__,_|\__\___|  \__,_|\__,_|\__\__,_|

nrns = 15
trials = 15
length = 600
state_order = np.asarray([0,1]) # use one order for all trials; len = num_transitions + 1
palatability_state = 1
ceil_p = 0.05 # Maximum firing probability -> Make shit sparse
jitter_t = 20 # Jitter from mean transition times for neurons on same trial
jitter_p = 0.0
jitter_p_type = 'abs'
min_duration = 100 # Min time of 1st transition & time b/w transitions & time of 2nd transition before end
seed_num = 30

all_ber_spikes, t, p, all_p, taste_scaling =  fake_ber_firing(nrns,
                    trials,
                    length,
                    state_order,
                    palatability_state,
                    ceil_p,
                    jitter_t,
                    jitter_p,
                    jitter_p_type,
                    min_duration)

all_ber_spikes = np.asarray(all_ber_spikes)
t = np.asarray(t)

all_ber_spikes_long = all_ber_spikes[0,:,:,:]
t_long = t[0,:,:,:]
for taste in range(1,all_ber_spikes.shape[0]):
    all_ber_spikes_long = np.concatenate((all_ber_spikes_long,all_ber_spikes[taste,:,:]),axis=1)
    t_long = np.concatenate((t_long,t[taste,:,:]),axis=0)

dat = np.asarray(all_ber_spikes)[0,:,0,:]
dat_t = t[0][0,:,:]
dot_raster(dat,dat_t)

# _____ _           _
#|  ___(_)_ __   __| |
#| |_  | | '_ \ / _` |
#|  _| | | | | | (_| |
#|_|   |_|_| |_|\__,_|
#
#      _                                        _       _
#  ___| |__   __ _ _ __   __ _  ___ _ __   ___ (_)_ __ | |_ ___
# / __| '_ \ / _` | '_ \ / _` |/ _ \ '_ \ / _ \| | '_ \| __/ __|
#| (__| | | | (_| | | | | (_| |  __/ |_) | (_) | | | | | |_\__ \
# \___|_| |_|\__,_|_| |_|\__, |\___| .__/ \___/|_|_| |_|\__|___/
#                        |___/     |_|

# =============================================================================
# Likelihood of generaeting data using single firing rate
# =============================================================================

# For every trial and neuron, walk through time and calculate the probability
# that the sum of spike up to that point was generated by a uniform firing rate
# using a binomial likelihood
log_probs = np.zeros(all_ber_spikes_long.shape)
mean_rate = np.mean(all_ber_spikes_long,axis=-1)
for trial in tqdm(range(log_probs.shape[1])):
# for trial in range(log_probs.shape[1]): # If tqdm not installed
    for nrn in range(log_probs.shape[0]):
        for time in range(1,log_probs.shape[-1]):
            sum_spikes = np.sum(all_ber_spikes_long[nrn,trial,:time])
            log_probs[nrn,trial,time] = binom.logpmf(sum_spikes,
                                     time,
                                     mean_rate[nrn,trial])

# Sum likelihood across the neuronal ensemble
# Each trial has a time-series of likelihood now
# Max value of negative likelihood should indicate changepoint
trial_log_probs = -np.sum(log_probs,axis=0)

# Plot out random subset of trials with actual changepoints and 
# time-series of likelhood
num_trials = 9
chosen_trials = np.random.randint(0,all_ber_spikes_long.shape[1],num_trials)
chosen_spikes = all_ber_spikes_long[:,chosen_trials,:]
chosen_t = t_long[chosen_trials,:,:]
chosen_probs = trial_log_probs[chosen_trials,:]

square_len = np.int(np.ceil(np.sqrt(num_trials)))
fig, ax = plt.subplots(square_len,square_len)

nd_idx_objs = []
for dim in range(ax.ndim):
    this_shape = np.ones(len(ax.shape))
    this_shape[dim] = ax.shape[dim]
    nd_idx_objs.append(np.broadcast_to( np.reshape(np.arange(ax.shape[dim]),this_shape.astype('int')), ax.shape).flatten())

for trial in range(num_trials):
    plt.sca(ax[nd_idx_objs[0][trial],nd_idx_objs[1][trial]])
    raster(chosen_spikes[:,trial,:],chosen_t[trial,:,:])
    plt.plot(nrns*chosen_probs[trial,:]/np.max(chosen_probs[trial,:]))



