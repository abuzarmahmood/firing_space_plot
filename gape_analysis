#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Oct 10 15:16:14 2018

@author: abuzarmahmood

   _____                                         _           _     
  / ____|                      /\               | |         (_)    
 | |  __  __ _ _ __   ___     /  \   _ __   __ _| |_   _ ___ _ ___ 
 | | |_ |/ _` | '_ \ / _ \   / /\ \ | '_ \ / _` | | | | / __| / __|
 | |__| | (_| | |_) |  __/  / ____ \| | | | (_| | | |_| \__ \ \__ \
  \_____|\__,_| .__/ \___| /_/    \_\_| |_|\__,_|_|\__, |___/_|___/
              | |                                   __/ |          
              |_|                                  |___/  

Attempting to correlate difference in baseline firing conditions
to gape onset times in different animals
This analysis attempts to relate baseline differences to HMM states
(which predict onset of gape timings) and in turn trial-to-trial variability
in the onset of HMM states.
"""
######################### Import dat ish #########################
import tables
import os
import numpy as np
import matplotlib.pyplot as plt

os.chdir('/media/bigdata/firing_space_plot/')
from ephys_data import ephys_data
from baseline_divergence_funcs import *
import multiprocessing as mp
import pandas as pd
import glob
import seaborn as sns

from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA as pca

from scipy.spatial import distance_matrix as dist_mat
from scipy.stats import pearsonr
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda

from skimage import exposure


#   _____      _     _____        _        
#  / ____|    | |   |  __ \      | |       
# | |  __  ___| |_  | |  | | __ _| |_ __ _ 
# | | |_ |/ _ \ __| | |  | |/ _` | __/ _` |
# | |__| |  __/ |_  | |__| | (_| | || (_| |
#  \_____|\___|\__| |_____/ \__,_|\__\__,_|
#

# Find all relevant files
#dir_list = ['/media/bigdata/Jenn_Data', '/media/bigdata/NM_2500/']
dir_list = ['/media/bigdata/jian_you_data/des_ic']
file_list = []
for x in dir_list:
    file_list = file_list + glob.glob(x + '/**/' + '*.h5',recursive=True)

# Iterate over files and calculate correlations with firing rates
corr_dat = pd.DataFrame()
for file in range(len(file_list)):
    
    ################ Indent everything after this
    
    # Load file and find corresponding directory
    data_dir = os.path.dirname(file_list[file])
    data = ephys_data(data_dir = data_dir ,file_id = file, use_chosen_units = True)
    data.firing_rate_params = dict(zip(['step_size','window_size','total_time'],
                                   [25,250,7000]))
    data.get_data()
    data.get_firing_rates()
    
    # Look at discriminability of population
    all_firing = data.all_normal_off_firing
    all_firing = all_firing[:,:,80:160]
    all_dists = np.zeros((all_firing.shape[1],all_firing.shape[1],all_firing.shape[2]))
    for time in range(all_firing.shape[2]):
        this_dist_mat = dist_mat(all_firing[:,:,time].T,all_firing[:,:,time].T)
        all_dists[:,:,time] = this_dist_mat
    
    plt.figure()
    plt.imshow(exposure.equalize_hist(np.mean(all_dists,axis=-1)))
    
    # Find most discriminative neurons
    all_firing_array = np.asarray(data.normal_off_firing)
    mean_all_firing = np.mean(np.mean(all_firing_array,axis=-1),axis=-1)
    
    # Use LDA to quantify discriminability of trials by the population
    all_firing_long = all_firing[0,:,:]
    for nrn in range(1,all_firing.shape[0]):
        all_firing_long = np.concatenate((all_firing_long,all_firing[int(nrn),:,:]),axis=1)
        
    all_firing_long_pca = pca(n_components = 30).fit(all_firing_long)
    explained_var = sum(all_firing_long_pca.explained_variance_ratio_)
    reduced_all = all_firing_long_pca.transform(all_firing_long)
    
    class_labels = []
    for i in range(4):
        class_labels = class_labels + [i]*15
        
    clf = lda()
    clf.fit(reduced_all, class_labels)
    
    accuracy = sum(clf.predict(reduced_all) == class_labels) / len(class_labels)
    print('explained_var = %.3f, accuracy = %.3f' % (explained_var,accuracy))
    ##########################
    
    data.correlation_params = dict(zip(['stimulus_start_time', 'stimulus_end_time',
                                        'baseline_start_time', 'baseline_end_time',
                                        'shuffle_repeats', 'accumulated'],
                                       [2000, 4000, 0, 2000, 100, False]))
    data.get_correlations()
# =============================================================================
#     data.get_dataframe()
#         
#     corr_dat = pd.concat([corr_dat, data.data_frame])
# =============================================================================
    print('file %i' % file)

for file in range(len(file_list)):        
    hf5 = tables.open_file(file_list[file])
    laser_combination = hf5.root.ancillary_analysis.laser_combination_d_l[:]

    #plt.figure()
    emg_freq = hf5.root.emg_BSA_results.omega[:]
    emg_prob = hf5.root.ancillary_analysis.emg_BSA_results[:]
    fin_gape_prob = np.sum(emg_prob[0,:,:,:,6:11],axis = -1)/np.sum(emg_prob[0,:,:,:,:],axis = -1)
    #plt.plot(np.mean(fin_gape_prob[:,:,2000:5000],axis=1).T)
    
    #emg_dist_mat = dist_mat(fin_gape_prob[taste,:,2000:5000],fin_gape_prob[taste,:,2000:5000])
    
    # Integrated Trial by trial gaping probabilities
    int_gape_probs = np.cumsum(fin_gape_prob[:,:,2000:4000],axis=-1)
# =============================================================================
#     mean_int_gape_probs = np.mean(int_gape_probs,axis=1)
#     for i in range(mean_int_gape_probs.shape[0]):
#         plt.errorbar(x = range(mean_int_gape_probs.shape[1]),
#                      y = mean_int_gape_probs[i,:],
#                      )#yerr = np.std(int_gape_probs,axis=1)[i,:])
# =============================================================================
        
    fin_int_gape_probs = int_gape_probs[:,:,-1]
    plt.hist([fin_int_gape_probs[1,:],fin_int_gape_probs[3,:]])
    
    # Find which neurons respond most to gaping
    
    off_pre_dists = copy.deepcopy(data.off_corr['pre_dists'])
    off_stim_dists = copy.deepcopy(data.off_corr['stim_dists'])
    
    off_stim_firing = copy.deepcopy(data.normal_off_firing)
    
    
    gape_dists = dist_mat(fin_int_gape_probs[taste,:][:,np.newaxis],
                          fin_int_gape_probs[taste,:][:,np.newaxis])
    indices = np.mask_indices(gape_dists[taste].shape[0], np.triu, 1)
    rho, p = pearsonr(gape_dists[indices], off_stim_dists[taste][indices])
    print([rho,p])
    
    
    
    plt.figure()
    plt.hist([fin_int_gape_probs[3,:], fin_int_gape_probs[1,:]])
    
    
    hf5.close()
    # Pick out gapes for no laser condition
    ### Pick only conc quinine and conc sucrose###
    gapes = hf5.root.ancillary_analysis.first_gape_Li[:][0]
    
    # Create distances for gapes
    gape_dists = []
    for taste in [1,3]:#range(len(gape_trials)):
        gape_dists.append(dist_mat(gapes[taste][:,np.newaxis],gapes[taste][:,np.newaxis]))
    
    # Find where gape numbers are not weird
    gape_trials = []    
    for x in gapes:
        gape_trials.append(np.where((x<10000) & (x>100))[0])
    
    
    fin_gapes = []
    # Select trials with gape onset 
    for taste in [1,3]:#range(len(gape_trials)):
        gape_dists[taste] = gape_dists[taste][gape_trials[taste],:]
        gape_dists[taste] = gape_dists[taste][:,gape_trials[taste]]
        off_pre_dists[taste] = off_pre_dists[taste][gape_trials[taste],:]
        off_pre_dists[taste] = off_pre_dists[taste][:,gape_trials[taste]]
        

    shuffle_num = 100
    rho_list = []
    rho_sh_list = []
    for taste in [1,3]:#range(len(gape_trials)):
        indices = np.mask_indices(gape_dists[taste].shape[0], np.triu, 1)
        rho, p = pearsonr(gape_dists[taste][indices], off_pre_dists[taste][indices])
        #plt.figure()
        #plt.subplot(2,1,1)
        #plt.imshow(exposure.equalize_hist(gape_dists[taste]))
        #plt.subplot(2,1,2)
        #plt.imshow(exposure.equalize_hist(off_pre_dists[taste]))
        rho_list.append(rho)
        
        this_rho_sh_list = []
        for i in range(shuffle_num):
            rho_sh, p_sh = pearsonr(np.random.permutation(gape_dists[taste][indices]), off_pre_dists[taste][indices])
            this_rho_sh_list.append(rho_sh)
        rho_sh_list.append(this_rho_sh_list)
        
        this_frame = pd.DataFrame({
                'file' : file,
                'taste' : taste,
                'shuffle' : False,
                'rho' : rho},
                index = [0])
        
        this_frame_sh = pd.DataFrame({
                'file' : file,
                'taste' : taste,
                'shuffle' : True,
                'rho' : this_rho_sh_list}) 
        
        corr_dat = pd.concat((corr_dat,this_frame,this_frame_sh))
        
    hf5.close()
       
sns.swarmplot(x = 'shuffle', y = 'rho', data = corr_dat)


    
    